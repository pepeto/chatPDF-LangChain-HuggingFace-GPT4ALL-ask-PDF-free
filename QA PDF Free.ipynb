{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db73ed7",
   "metadata": {},
   "source": [
    "# QA PDF Free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d4ae366",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T19:10:17.324650Z",
     "start_time": "2023-05-13T19:10:16.099651Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6518fd8e",
   "metadata": {},
   "source": [
    "## Loads PDF and Splits into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95400cce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T19:10:17.339648Z",
     "start_time": "2023-05-13T19:10:17.326651Z"
    }
   },
   "outputs": [],
   "source": [
    "# It took 8 minutes to get a 350 pages pdf.\n",
    "#loader = UnstructuredPDFLoader(\".\\\\MyPDF.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c68c5eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T19:10:17.355648Z",
     "start_time": "2023-05-13T19:10:17.340650Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GutenbergLoader\n",
    "\n",
    "loader = GutenbergLoader('https://www.gutenberg.org/cache/epub/30155/pg30155.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "777e6758",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T19:10:18.391976Z",
     "start_time": "2023-05-13T19:10:17.357649Z"
    }
   },
   "outputs": [],
   "source": [
    "#pages = loader.load_and_split() # Cuts automatically in 4000 caracters.\n",
    "pages = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(    \n",
    "    chunk_size=350, # the maximum size of your chunks (as measured by the length function).\n",
    "    chunk_overlap=100 # the maximum overlap between chunks.\n",
    "    #length_function = len # how the length of chunks is calculated. Defaults: counting # of chars, or token counter\n",
    ")\n",
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f2f064",
   "metadata": {},
   "source": [
    "## Creates Embeddings and Stores them in Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc3a8752",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T19:10:27.662791Z",
     "start_time": "2023-05-13T19:10:18.392975Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'} # cuda if you have GPU\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name, \n",
    "    model_kwargs=model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "609e1c27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T19:11:17.141850Z",
     "start_time": "2023-05-13T19:10:27.663792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chroma using direct local API.\n",
      "Using DuckDB in-memory for database. Data will be transient.\n"
     ]
    }
   ],
   "source": [
    "docsearch = Chroma.from_documents(docs, embeddings).as_retriever() # para despues usar docsearch.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fe995d",
   "metadata": {},
   "source": [
    "## Search for the most relevant chunks for the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74b81d5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T19:14:06.336490Z",
     "start_time": "2023-05-13T19:14:06.295492Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"What Hubble discovered?\"\n",
    "question = query\n",
    "# Looks for the more representative chunks in Chroma db.\n",
    "docs = docsearch.get_relevant_documents(query)\n",
    "#docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd534c09",
   "metadata": {},
   "source": [
    "# Put all the pieces together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6750232b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T19:14:29.526918Z",
     "start_time": "2023-05-13T19:14:29.511919Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin\n",
    "local_path = 'm:\\GPT4ALL\\MODELS\\ggml-gpt4all-l13b-snoozy.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "465ca5f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T19:14:36.954317Z",
     "start_time": "2023-05-13T19:14:36.543373Z"
    }
   },
   "outputs": [],
   "source": [
    "llm = GPT4All(\n",
    "    model=local_path, \n",
    "    #n_ctx=512\n",
    "    verbose=False, \n",
    "    #n_threads=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e7993f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T19:14:37.172589Z",
     "start_time": "2023-05-13T19:14:37.159587Z"
    }
   },
   "outputs": [],
   "source": [
    "#llm_chain = LLMChain(prompt=prompt, llm=llm) # , max_new_tokens=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72842a30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T19:14:38.276442Z",
     "start_time": "2023-05-13T19:14:37.680443Z"
    }
   },
   "outputs": [],
   "source": [
    "llm_chain = load_qa_chain(llm, chain_type=\"stuff\") # , prompt=prompt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3115e723",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T19:16:53.182198Z",
     "start_time": "2023-05-13T19:14:38.277443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': ' He found evidence for a redshift in extra-galactic nebulae (“milky ways”), which can be interpreted as an expansion based on present knowledge. The interpretation confirms his theory, but there is also some difficulty with the origin of this expansion dating back only about 109 years ago from a theoretical point of view while physical astronomy suggests it could have occurred much earlier or even indefinitely far into past time periods.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f915e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1633d4fe",
   "metadata": {},
   "source": [
    "GPT4ALL parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91829ba5",
   "metadata": {},
   "source": [
    "    field echo: Optional[bool] = False\n",
    "    Whether to echo the prompt.\n",
    "\n",
    "    field embedding: bool = False\n",
    "    Use embedding mode only.\n",
    "\n",
    "    field f16_kv: bool = False\n",
    "    Use half-precision for key/value cache.\n",
    "\n",
    "    field logits_all: bool = False\n",
    "    Return logits for all tokens, not just the last token.\n",
    "\n",
    "    field model: str [Required]\n",
    "    Path to the pre-trained GPT4All model file.\n",
    "\n",
    "    field n_batch: int = 1\n",
    "    Batch size for prompt processing.\n",
    "\n",
    "    field n_ctx: int = 512\n",
    "    Token context window.\n",
    "\n",
    "    field n_parts: int = -1\n",
    "    Number of parts to split the model into. If -1, the number of parts is automatically determined.\n",
    "\n",
    "    field n_predict: Optional[int] = 256\n",
    "    The maximum number of tokens to generate.\n",
    "\n",
    "    field n_threads: Optional[int] = 4\n",
    "    Number of threads to use.\n",
    "\n",
    "    field repeat_last_n: Optional[int] = 64\n",
    "    Last n tokens to penalize\n",
    "\n",
    "    field repeat_penalty: Optional[float] = 1.3\n",
    "    The penalty to apply to repeated tokens.\n",
    "\n",
    "    field seed: int = 0\n",
    "    Seed. If -1, a random seed is used.\n",
    "\n",
    "    field stop: Optional[List[str]] = []\n",
    "    A list of strings to stop generation when encountered.\n",
    "\n",
    "    field streaming: bool = False\n",
    "    Whether to stream the results or not.\n",
    "\n",
    "    field temp: Optional[float] = 0.8\n",
    "    The temperature to use for sampling.\n",
    "\n",
    "    field top_k: Optional[int] = 40\n",
    "    The top-k value to use for sampling.\n",
    "\n",
    "    field top_p: Optional[float] = 0.95\n",
    "    The top-p value to use for sampling.\n",
    "\n",
    "    field use_mlock: bool = False\n",
    "    Force system to keep model in RAM.\n",
    "\n",
    "    field verbose: bool [Optional]\n",
    "    Whether to print out response text.\n",
    "\n",
    "    field vocab_only: bool = False\n",
    "    Only load the vocabulary, no weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bbe034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd63d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16baca71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
